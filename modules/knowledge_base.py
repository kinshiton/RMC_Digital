"""
GuardNova çŸ¥è¯†åº“ç³»ç»Ÿ
æ”¯æŒ RAGï¼ˆRetrieval Augmented Generationï¼‰æ™ºèƒ½æ£€ç´¢
æ”¯æŒ Supabase äº‘æ•°æ®åº“æŒä¹…åŒ–å­˜å‚¨
"""

import sqlite3
import json
from datetime import datetime
from pathlib import Path
import hashlib
from typing import List, Dict, Optional
import requests
from bs4 import BeautifulSoup
import pandas as pd
from docx import Document
import PyPDF2
import numpy as np
import os

# å¯¼å…¥ Supabase é€‚é…å™¨
try:
    from .supabase_adapter import get_supabase_adapter
    SUPABASE_SUPPORT = True
except:
    SUPABASE_SUPPORT = False

class KnowledgeBase:
    def __init__(self, db_path: str = "data/knowledge_base.db"):
        self.db_path = Path(db_path)
        self.db_path.parent.mkdir(parents=True, exist_ok=True)
        self._init_database()
        
        # åˆå§‹åŒ– Supabase æ”¯æŒ
        self.supabase = None
        if SUPABASE_SUPPORT:
            try:
                self.supabase = get_supabase_adapter()
                if self.supabase.enabled:
                    print("âœ… çŸ¥è¯†åº“å·²è¿æ¥åˆ° Supabase äº‘æ•°æ®åº“")
            except Exception as e:
                print(f"âš ï¸ Supabase åˆå§‹åŒ–å¤±è´¥ï¼Œä½¿ç”¨æœ¬åœ°æ•°æ®åº“: {e}")
    
    def _init_database(self):
        """åˆå§‹åŒ–æ•°æ®åº“"""
        conn = sqlite3.connect(self.db_path)
        cursor = conn.cursor()
        
        # çŸ¥è¯†åº“ä¸»è¡¨
        cursor.execute("""
        CREATE TABLE IF NOT EXISTS knowledge_items (
            id INTEGER PRIMARY KEY AUTOINCREMENT,
            title TEXT NOT NULL,
            content TEXT NOT NULL,
            content_type TEXT NOT NULL,
            file_path TEXT,
            external_url TEXT,
            tags TEXT,
            embedding_vector TEXT,
            created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
            updated_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
            last_crawled_at TIMESTAMP
        )
        """)
        
        # åˆ›å»ºç´¢å¼•
        cursor.execute("""
        CREATE INDEX IF NOT EXISTS idx_content_type 
        ON knowledge_items(content_type)
        """)
        
        cursor.execute("""
        CREATE INDEX IF NOT EXISTS idx_tags 
        ON knowledge_items(tags)
        """)
        
        conn.commit()
        conn.close()
    
    def add_text_knowledge(self, title: str, content: str, tags: str = "") -> int:
        """æ·»åŠ æ–‡æœ¬çŸ¥è¯†å¹¶è‡ªåŠ¨ç”Ÿæˆembedding"""
        # 1. ä¿å­˜åˆ°æœ¬åœ° SQLite
        conn = sqlite3.connect(self.db_path)
        cursor = conn.cursor()
        
        cursor.execute("""
        INSERT INTO knowledge_items (title, content, content_type, tags)
        VALUES (?, ?, ?, ?)
        """, (title, content, "text", tags))
        
        knowledge_id = cursor.lastrowid
        conn.commit()
        conn.close()
        
        # 2. åŒæ­¥åˆ° Supabase äº‘æ•°æ®åº“
        if self.supabase and self.supabase.enabled:
            try:
                self.supabase.add_knowledge_item(title, content, "text", tags=tags)
                print("âœ… çŸ¥è¯†å·²åŒæ­¥åˆ°äº‘ç«¯")
            except Exception as e:
                print(f"âš ï¸ äº‘ç«¯åŒæ­¥å¤±è´¥ï¼ˆæœ¬åœ°å·²ä¿å­˜ï¼‰: {e}")
        
        # 3. å¼‚æ­¥ç”Ÿæˆ embeddingï¼ˆä¸é˜»å¡ä¸»æµç¨‹ï¼‰
        try:
            self.update_embedding(knowledge_id)
        except Exception as e:
            print(f"ç”Ÿæˆ embedding å¤±è´¥ï¼ˆä¸å½±å“ä¿å­˜ï¼‰: {e}")
        
        return knowledge_id
    
    def add_file_knowledge(self, title: str, file_path: str, description: str = "", tags: str = "") -> int:
        """æ·»åŠ æ–‡ä»¶çŸ¥è¯†å¹¶è§£æå†…å®¹ï¼Œè‡ªåŠ¨ç”Ÿæˆembedding"""
        conn = sqlite3.connect(self.db_path)
        cursor = conn.cursor()
        
        # è§£ææ–‡ä»¶å†…å®¹
        content = self._parse_file_content(file_path, description)
        
        cursor.execute("""
        INSERT INTO knowledge_items (title, content, content_type, file_path, tags)
        VALUES (?, ?, ?, ?, ?)
        """, (title, content, "file", file_path, tags))
        
        knowledge_id = cursor.lastrowid
        conn.commit()
        conn.close()
        
        # å¼‚æ­¥ç”Ÿæˆ embedding
        try:
            self.update_embedding(knowledge_id)
        except Exception as e:
            print(f"ç”Ÿæˆ embedding å¤±è´¥ï¼ˆä¸å½±å“ä¿å­˜ï¼‰: {e}")
        
        return knowledge_id
    
    def _parse_file_content(self, file_path: str, description: str = "") -> str:
        """è§£ææ–‡ä»¶å†…å®¹ä¸ºMarkdownæ ¼å¼"""
        try:
            file_path_obj = Path(file_path)
            suffix = file_path_obj.suffix.lower()
            
            # Excel æ–‡ä»¶
            if suffix in ['.xlsx', '.xls', '.csv']:
                return self._parse_excel(file_path_obj, description)
            
            # Word æ–‡ä»¶
            elif suffix in ['.docx', '.doc']:
                return self._parse_word(file_path_obj, description)
            
            # PDF æ–‡ä»¶
            elif suffix == '.pdf':
                return self._parse_pdf(file_path_obj, description)
            
            # æ–‡æœ¬æ–‡ä»¶
            elif suffix in ['.txt', '.md']:
                with open(file_path, 'r', encoding='utf-8') as f:
                    content = f.read()
                if description:
                    return f"{description}\n\n{content}"
                return content
            
            else:
                return description or f"æ–‡ä»¶ï¼š{file_path_obj.name}"
        
        except Exception as e:
            print(f"è§£ææ–‡ä»¶å¤±è´¥ï¼š{e}")
            return description or f"æ–‡ä»¶ï¼š{file_path}"
    
    def _parse_excel(self, file_path: Path, description: str = "") -> str:
        """è§£æExcelæ–‡ä»¶ä¸ºMarkdownè¡¨æ ¼"""
        try:
            # è¯»å–Excelæ–‡ä»¶
            if file_path.suffix == '.csv':
                df = pd.read_csv(file_path)
            else:
                df = pd.read_excel(file_path)
            
            # è½¬æ¢ä¸ºMarkdownæ ¼å¼
            markdown_parts = []
            
            if description:
                markdown_parts.append(f"{description}\n")
            
            markdown_parts.append(f"**æ–‡ä»¶åï¼š** {file_path.name}")
            markdown_parts.append(f"**æ•°æ®è¡Œæ•°ï¼š** {len(df)} è¡Œ")
            markdown_parts.append(f"**åˆ—æ•°ï¼š** {len(df.columns)} åˆ—")
            markdown_parts.append(f"**åˆ—åï¼š** {', '.join(df.columns.tolist())}\n")
            
            # è½¬æ¢ä¸ºMarkdownè¡¨æ ¼ï¼ˆé™åˆ¶å‰20è¡Œï¼Œé¿å…å†…å®¹è¿‡é•¿ï¼‰
            display_rows = min(20, len(df))
            markdown_table = df.head(display_rows).to_markdown(index=False)
            
            markdown_parts.append("**æ•°æ®å†…å®¹ï¼š**\n")
            markdown_parts.append(markdown_table)
            
            if len(df) > display_rows:
                markdown_parts.append(f"\n*ï¼ˆè¡¨æ ¼å…± {len(df)} è¡Œï¼Œä»¥ä¸Šæ˜¾ç¤ºå‰ {display_rows} è¡Œï¼‰*")
            
            # æ·»åŠ æ•°æ®æ‘˜è¦
            markdown_parts.append("\n**æ•°æ®æ‘˜è¦ï¼š**")
            for col in df.columns[:5]:  # åªæ˜¾ç¤ºå‰5åˆ—çš„æ‘˜è¦
                if df[col].dtype in ['int64', 'float64']:
                    markdown_parts.append(f"- {col}: æ•°å€¼èŒƒå›´ {df[col].min()} ~ {df[col].max()}")
                else:
                    unique_count = df[col].nunique()
                    markdown_parts.append(f"- {col}: {unique_count} ä¸ªä¸åŒå€¼")
            
            return "\n".join(markdown_parts)
        
        except Exception as e:
            return f"{description}\n\næ–‡ä»¶è§£æå¤±è´¥ï¼š{str(e)}" if description else f"Excelæ–‡ä»¶ï¼š{file_path.name}ï¼ˆè§£æå¤±è´¥ï¼‰"
    
    def _parse_word(self, file_path: Path, description: str = "") -> str:
        """è§£æWordæ–‡æ¡£å†…å®¹"""
        try:
            doc = Document(file_path)
            
            markdown_parts = []
            
            if description:
                markdown_parts.append(f"{description}\n")
            
            markdown_parts.append(f"**æ–‡ä»¶åï¼š** {file_path.name}\n")
            
            # æå–æ–‡æœ¬å†…å®¹
            full_text = []
            for para in doc.paragraphs:
                if para.text.strip():
                    full_text.append(para.text)
            
            if full_text:
                markdown_parts.append("\n**æ–‡æ¡£å†…å®¹ï¼š**\n")
                markdown_parts.append("\n\n".join(full_text))
            
            return "\n".join(markdown_parts)
        
        except Exception as e:
            return f"{description}\n\næ–‡ä»¶è§£æå¤±è´¥ï¼š{str(e)}" if description else f"Wordæ–‡æ¡£ï¼š{file_path.name}ï¼ˆè§£æå¤±è´¥ï¼‰"
    
    def _parse_pdf(self, file_path: Path, description: str = "") -> str:
        """è§£æPDFæ–‡ä»¶å†…å®¹"""
        try:
            markdown_parts = []
            
            if description:
                markdown_parts.append(f"{description}\n")
            
            markdown_parts.append(f"**æ–‡ä»¶åï¼š** {file_path.name}\n")
            
            # è¯»å–PDF
            with open(file_path, 'rb') as f:
                reader = PyPDF2.PdfReader(f)
                num_pages = len(reader.pages)
                
                markdown_parts.append(f"**é¡µæ•°ï¼š** {num_pages}\n")
                
                # æå–å‰10é¡µæ–‡æœ¬
                text_parts = []
                for i in range(min(10, num_pages)):
                    page = reader.pages[i]
                    text = page.extract_text()
                    if text.strip():
                        text_parts.append(f"### ç¬¬ {i+1} é¡µ\n{text}")
                
                if text_parts:
                    markdown_parts.append("\n**æ–‡æ¡£å†…å®¹ï¼š**\n")
                    markdown_parts.append("\n\n".join(text_parts))
                
                if num_pages > 10:
                    markdown_parts.append(f"\n\n*ï¼ˆå…± {num_pages} é¡µï¼Œä»…æ˜¾ç¤ºå‰10é¡µï¼‰*")
            
            return "\n".join(markdown_parts)
        
        except Exception as e:
            return f"{description}\n\næ–‡ä»¶è§£æå¤±è´¥ï¼š{str(e)}" if description else f"PDFæ–‡ä»¶ï¼š{file_path.name}ï¼ˆè§£æå¤±è´¥ï¼‰"
    
    def add_url_knowledge(self, title: str, url: str, description: str = "", tags: str = "") -> int:
        """æ·»åŠ é“¾æ¥çŸ¥è¯†ï¼ˆRAG ç½‘é¡µçˆ¬å–ï¼‰ï¼Œè‡ªåŠ¨ç”Ÿæˆembedding"""
        conn = sqlite3.connect(self.db_path)
        cursor = conn.cursor()
        
        # çˆ¬å–ç½‘é¡µå†…å®¹
        content = self._crawl_webpage(url)
        if not content:
            content = description or f"é“¾æ¥ï¼š{url}"
        
        cursor.execute("""
        INSERT INTO knowledge_items (title, content, content_type, external_url, tags, last_crawled_at)
        VALUES (?, ?, ?, ?, ?, ?)
        """, (title, content, "url", url, tags, datetime.now()))
        
        knowledge_id = cursor.lastrowid
        conn.commit()
        conn.close()
        
        # å¼‚æ­¥ç”Ÿæˆ embedding
        try:
            self.update_embedding(knowledge_id)
        except Exception as e:
            print(f"ç”Ÿæˆ embedding å¤±è´¥ï¼ˆä¸å½±å“ä¿å­˜ï¼‰: {e}")
        
        return knowledge_id
    
    def _crawl_webpage(self, url: str) -> str:
        """çˆ¬å–ç½‘é¡µå†…å®¹ï¼ˆç®€åŒ–ç‰ˆ RAGï¼‰"""
        try:
            headers = {
                'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'
            }
            response = requests.get(url, headers=headers, timeout=10)
            response.raise_for_status()
            
            soup = BeautifulSoup(response.content, 'html.parser')
            
            # ç§»é™¤è„šæœ¬å’Œæ ·å¼
            for script in soup(["script", "style"]):
                script.decompose()
            
            # æå–æ–‡æœ¬
            text = soup.get_text(separator='\n', strip=True)
            
            # æ¸…ç†ç©ºè¡Œ
            lines = [line.strip() for line in text.split('\n') if line.strip()]
            content = '\n'.join(lines[:100])  # é™åˆ¶é•¿åº¦
            
            return content
        except Exception as e:
            print(f"çˆ¬å–ç½‘é¡µå¤±è´¥ï¼š{e}")
            return ""
    
    def refresh_url_knowledge(self, knowledge_id: int) -> bool:
        """åˆ·æ–°é“¾æ¥çŸ¥è¯†ï¼ˆå®šæ—¶æ›´æ–°ï¼‰"""
        conn = sqlite3.connect(self.db_path)
        cursor = conn.cursor()
        
        cursor.execute("""
        SELECT external_url FROM knowledge_items WHERE id = ? AND content_type = 'url'
        """, (knowledge_id,))
        
        result = cursor.fetchone()
        if not result:
            conn.close()
            return False
        
        url = result[0]
        new_content = self._crawl_webpage(url)
        
        if new_content:
            cursor.execute("""
            UPDATE knowledge_items 
            SET content = ?, updated_at = ?, last_crawled_at = ?
            WHERE id = ?
            """, (new_content, datetime.now(), datetime.now(), knowledge_id))
            
            conn.commit()
            conn.close()
            return True
        
        conn.close()
        return False
    
    def search_knowledge(self, query: str, limit: int = 5) -> List[Dict]:
        """æ™ºèƒ½æœç´¢çŸ¥è¯†åº“ï¼ˆç®€åŒ–ç‰ˆ RAGï¼‰"""
        conn = sqlite3.connect(self.db_path)
        cursor = conn.cursor()
        
        # æ”¹è¿›çš„åˆ†è¯ç­–ç•¥ï¼šè¿‡æ»¤åœç”¨è¯ï¼Œæå–å…³é”®è¯
        stopwords = {'æ˜¯', 'çš„', 'äº†', 'åœ¨', 'æœ‰', 'å’Œ', 'å°±', 'ä¸', 'äºº', 'æˆ‘', 'ä»–', 'å¥¹', 'ä»¬',
                     'ä»€ä¹ˆ', 'æ€ä¹ˆ', 'å¦‚ä½•', 'ä¸ºä»€ä¹ˆ', 'å—', 'å‘¢', 'å•Š', 'å§', 'ä¹ˆ', 'å“ª', 'å“ªé‡Œ',
                     'ä¸€ä¸ª', 'è¿™ä¸ª', 'é‚£ä¸ª', 'è¿™äº›', 'é‚£äº›', 'ä»€', 'ä¹ˆ', 'æ„æ€', 'å®šä¹‰'}
        
        # åˆ†è¯å¹¶è¿‡æ»¤
        words = query.lower().split()
        keywords = [w for w in words if w not in stopwords and len(w) > 1]
        
        # å¦‚æœè¿‡æ»¤åæ²¡æœ‰å…³é”®è¯ï¼Œä½¿ç”¨åŸå§‹æŸ¥è¯¢
        if not keywords:
            keywords = [query.lower()]
        
        # æ„å»ºæœç´¢æ¡ä»¶ï¼ˆä½¿ç”¨ ORï¼Œä½†æ¯ä¸ªå…³é”®è¯éƒ½è¦åŒ¹é…åˆ°æ ‡é¢˜ã€å†…å®¹æˆ–æ ‡ç­¾ï¼‰
        search_conditions = []
        search_params = []
        
        for keyword in keywords:
            search_conditions.append(
                "(LOWER(title) LIKE ? OR LOWER(content) LIKE ? OR LOWER(tags) LIKE ?)"
            )
            search_params.extend([f"%{keyword}%", f"%{keyword}%", f"%{keyword}%"])
        
        where_clause = " OR ".join(search_conditions)
        
        cursor.execute(f"""
        SELECT id, title, content, content_type, file_path, external_url, tags, created_at
        FROM knowledge_items
        WHERE {where_clause}
        ORDER BY updated_at DESC
        LIMIT ?
        """, search_params + [limit])
        
        results = []
        for row in cursor.fetchall():
            results.append({
                'id': row[0],
                'title': row[1],
                'content': row[2],
                'content_type': row[3],
                'file_path': row[4],
                'external_url': row[5],
                'tags': row[6],
                'created_at': row[7]
            })
        
        conn.close()
        return results
    
    def get_all_knowledge(self) -> List[Dict]:
        """è·å–æ‰€æœ‰çŸ¥è¯†"""
        conn = sqlite3.connect(self.db_path)
        cursor = conn.cursor()
        
        cursor.execute("""
        SELECT id, title, content, content_type, file_path, external_url, tags, created_at
        FROM knowledge_items
        ORDER BY created_at DESC
        """)
        
        results = []
        for row in cursor.fetchall():
            results.append({
                'id': row[0],
                'title': row[1],
                'content': row[2],
                'content_type': row[3],
                'file_path': row[4],
                'external_url': row[5],
                'tags': row[6],
                'created_at': row[7]
            })
        
        conn.close()
        return results
    
    def delete_knowledge(self, knowledge_id: int) -> bool:
        """åˆ é™¤çŸ¥è¯†"""
        conn = sqlite3.connect(self.db_path)
        cursor = conn.cursor()
        
        cursor.execute("DELETE FROM knowledge_items WHERE id = ?", (knowledge_id,))
        
        success = cursor.rowcount > 0
        conn.commit()
        conn.close()
        
        return success
    
    def generate_rag_response(self, query: str, ai_client, model: str = "deepseek-chat") -> str:
        """ä½¿ç”¨ RAG ç”Ÿæˆå›ç­”"""
        # 1. æœç´¢çŸ¥è¯†åº“
        search_results = self.search_knowledge(query, limit=3)
        
        # 2. å¦‚æœçŸ¥è¯†åº“ä¸­æœ‰ç›¸å…³å†…å®¹ï¼Œä½¿ç”¨ RAG
        if search_results:
            context = "\n\n".join([
                f"çŸ¥è¯† {i+1}ï¼š{item['title']}\n{item['content'][:500]}"
                for i, item in enumerate(search_results)
            ])
            
            system_prompt = f"""ä½ æ˜¯ GuardNova AI æ™ºèƒ½åŠ©æ‰‹ã€‚

ä»¥ä¸‹æ˜¯ä»çŸ¥è¯†åº“ä¸­æ£€ç´¢åˆ°çš„ç›¸å…³ä¿¡æ¯ï¼š

{context}

è¯·åŸºäºä¸Šè¿°çŸ¥è¯†åº“å†…å®¹å›ç­”ç”¨æˆ·çš„é—®é¢˜ã€‚å¦‚æœçŸ¥è¯†åº“å†…å®¹ä¸é—®é¢˜ç›¸å…³ï¼Œè¯·ä¼˜å…ˆä½¿ç”¨çŸ¥è¯†åº“ä¿¡æ¯ï¼›å¦‚æœçŸ¥è¯†åº“å†…å®¹ä¸è¶³ä»¥å›ç­”é—®é¢˜ï¼Œè¯·ç»“åˆä½ çš„çŸ¥è¯†è¿›è¡Œè¡¥å……è¯´æ˜ã€‚

è¯·ç”¨å‹å¥½ã€ä¸“ä¸šçš„è¯­æ°”å›ç­”ï¼Œå¹¶åœ¨é€‚å½“æ—¶æ ‡æ³¨ä¿¡æ¯æ¥æºï¼ˆçŸ¥è¯†åº“æˆ–é€šç”¨çŸ¥è¯†ï¼‰ã€‚"""
        else:
            # 3. çŸ¥è¯†åº“ä¸­æ²¡æœ‰ç›¸å…³å†…å®¹ï¼Œä½¿ç”¨é€šç”¨ AI
            system_prompt = "ä½ æ˜¯ GuardNovaï¼Œä¸€ä¸ªä¸“ä¸šã€å‹å¥½çš„ AI æ™ºèƒ½åŠ©æ‰‹ã€‚"
        
        # 4. è°ƒç”¨ AI ç”Ÿæˆå›ç­”
        try:
            messages = [
                {"role": "system", "content": system_prompt},
                {"role": "user", "content": query}
            ]
            
            response = ai_client.chat.completions.create(
                model=model,
                messages=messages,
                temperature=0.7,
                max_tokens=2000
            )
            
            answer = response.choices[0].message.content
            
            # 5. æ·»åŠ çŸ¥è¯†åº“æ¥æºæ ‡æ³¨
            if search_results:
                sources = "\n\n---\nğŸ“š **å‚è€ƒçŸ¥è¯†ï¼š**\n" + "\n".join([
                    f"- {item['title']}" for item in search_results
                ])
                answer += sources
            
            return answer
        except Exception as e:
            return f"æŠ±æ­‰ï¼Œç”Ÿæˆå›ç­”æ—¶å‡ºç°é”™è¯¯ï¼š{str(e)}"
    
    # ============ å‘é‡æœç´¢åŠŸèƒ½ (Embeddings) ============
    
    def _get_openai_client(self):
        """åŠ¨æ€å¯¼å…¥å¹¶åˆå§‹åŒ– OpenAI å®¢æˆ·ç«¯ï¼ˆç”¨äº embeddingï¼‰"""
        try:
            import importlib
            openai_module = importlib.import_module('openai')
            
            # è·å– API Key - ä¼˜å…ˆä½¿ç”¨ OPENAI_API_KEYï¼ˆå› ä¸º embedding éœ€è¦ï¼‰
            api_key = os.getenv('OPENAI_API_KEY') or os.getenv('DEEPSEEK_API_KEY')
            if not api_key:
                return None
            
            # ä½¿ç”¨æ ‡å‡† OpenAI APIï¼ˆembedding åŠŸèƒ½ï¼‰
            client = openai_module.OpenAI(api_key=api_key)
            return client
        except Exception as e:
            print(f"åˆå§‹åŒ– OpenAI å®¢æˆ·ç«¯å¤±è´¥: {e}")
            return None
    
    def generate_embedding(self, text: str) -> Optional[List[float]]:
        """
        ç”Ÿæˆæ–‡æœ¬çš„ embedding å‘é‡
        ä½¿ç”¨ DeepSeek çš„ embedding æ¨¡å‹
        """
        try:
            client = self._get_openai_client()
            if not client:
                print("æ— æ³•åˆå§‹åŒ– OpenAI å®¢æˆ·ç«¯ï¼Œè·³è¿‡ embedding ç”Ÿæˆ")
                return None
            
            # æˆªæ–­è¿‡é•¿çš„æ–‡æœ¬ï¼ˆembedding æ¨¡å‹é€šå¸¸æœ‰é•¿åº¦é™åˆ¶ï¼‰
            max_length = 8000  # DeepSeek embedding æ¨¡å‹çš„æœ€å¤§é•¿åº¦
            if len(text) > max_length:
                text = text[:max_length]
            
            # è°ƒç”¨ embedding API
            response = client.embeddings.create(
                model="text-embedding-ada-002",  # æˆ–ä½¿ç”¨ DeepSeek çš„ embedding æ¨¡å‹
                input=text
            )
            
            embedding = response.data[0].embedding
            return embedding
        
        except Exception as e:
            print(f"ç”Ÿæˆ embedding å¤±è´¥: {e}")
            return None
    
    def cosine_similarity(self, vec1: List[float], vec2: List[float]) -> float:
        """
        è®¡ç®—ä¸¤ä¸ªå‘é‡çš„ä½™å¼¦ç›¸ä¼¼åº¦
        è¿”å›å€¼èŒƒå›´: -1 åˆ° 1ï¼Œè¶Šæ¥è¿‘ 1 è¡¨ç¤ºè¶Šç›¸ä¼¼
        """
        try:
            vec1 = np.array(vec1)
            vec2 = np.array(vec2)
            
            dot_product = np.dot(vec1, vec2)
            norm1 = np.linalg.norm(vec1)
            norm2 = np.linalg.norm(vec2)
            
            if norm1 == 0 or norm2 == 0:
                return 0.0
            
            return float(dot_product / (norm1 * norm2))
        except Exception as e:
            print(f"è®¡ç®—ä½™å¼¦ç›¸ä¼¼åº¦å¤±è´¥: {e}")
            return 0.0
    
    def update_embedding(self, item_id: int) -> bool:
        """
        ä¸ºæŒ‡å®šçš„çŸ¥è¯†æ¡ç›®ç”Ÿæˆå¹¶æ›´æ–° embedding
        """
        try:
            conn = sqlite3.connect(self.db_path)
            cursor = conn.cursor()
            
            # è·å–çŸ¥è¯†æ¡ç›®
            cursor.execute("""
            SELECT title, content FROM knowledge_items WHERE id = ?
            """, (item_id,))
            
            row = cursor.fetchone()
            if not row:
                conn.close()
                return False
            
            title, content = row
            
            # ç”Ÿæˆ embeddingï¼ˆæ ‡é¢˜ + å†…å®¹ï¼‰
            text_to_embed = f"{title}\n{content}"
            embedding = self.generate_embedding(text_to_embed)
            
            if embedding is None:
                conn.close()
                return False
            
            # å­˜å‚¨ä¸º JSON å­—ç¬¦ä¸²
            embedding_json = json.dumps(embedding)
            
            # æ›´æ–°æ•°æ®åº“
            cursor.execute("""
            UPDATE knowledge_items 
            SET embedding_vector = ?, updated_at = CURRENT_TIMESTAMP
            WHERE id = ?
            """, (embedding_json, item_id))
            
            conn.commit()
            conn.close()
            
            print(f"âœ… å·²ä¸ºçŸ¥è¯†æ¡ç›® {item_id} ç”Ÿæˆ embedding")
            return True
        
        except Exception as e:
            print(f"æ›´æ–° embedding å¤±è´¥: {e}")
            return False
    
    def update_all_embeddings(self) -> Dict[str, int]:
        """
        ä¸ºæ‰€æœ‰æ²¡æœ‰ embedding çš„çŸ¥è¯†æ¡ç›®ç”Ÿæˆå‘é‡
        è¿”å›ç»Ÿè®¡ä¿¡æ¯
        """
        try:
            conn = sqlite3.connect(self.db_path)
            cursor = conn.cursor()
            
            # æŸ¥æ‰¾æ‰€æœ‰æ²¡æœ‰ embedding çš„æ¡ç›®
            cursor.execute("""
            SELECT id FROM knowledge_items 
            WHERE embedding_vector IS NULL OR embedding_vector = ''
            """)
            
            items_to_update = cursor.fetchall()
            conn.close()
            
            success_count = 0
            fail_count = 0
            
            for (item_id,) in items_to_update:
                if self.update_embedding(item_id):
                    success_count += 1
                else:
                    fail_count += 1
            
            return {
                'total': len(items_to_update),
                'success': success_count,
                'failed': fail_count
            }
        
        except Exception as e:
            print(f"æ‰¹é‡æ›´æ–° embeddings å¤±è´¥: {e}")
            return {'total': 0, 'success': 0, 'failed': 0}
    
    def vector_search(self, query: str, limit: int = 5, threshold: float = 0.5) -> List[Dict]:
        """
        åŸºäºå‘é‡ç›¸ä¼¼åº¦çš„è¯­ä¹‰æœç´¢
        
        Args:
            query: æŸ¥è¯¢æ–‡æœ¬
            limit: è¿”å›ç»“æœæ•°é‡
            threshold: ç›¸ä¼¼åº¦é˜ˆå€¼ï¼ˆ0-1ï¼‰ï¼Œä½äºæ­¤å€¼çš„ç»“æœä¼šè¢«è¿‡æ»¤
        
        Returns:
            æŒ‰ç›¸ä¼¼åº¦æ’åºçš„çŸ¥è¯†æ¡ç›®åˆ—è¡¨
        """
        try:
            # 1. ç”ŸæˆæŸ¥è¯¢çš„ embedding
            query_embedding = self.generate_embedding(query)
            if query_embedding is None:
                print("æ— æ³•ç”ŸæˆæŸ¥è¯¢ embeddingï¼Œå›é€€åˆ°å…³é”®è¯æœç´¢")
                return self.search_knowledge(query, limit)
            
            # 2. è·å–æ‰€æœ‰æœ‰ embedding çš„çŸ¥è¯†æ¡ç›®
            conn = sqlite3.connect(self.db_path)
            cursor = conn.cursor()
            
            cursor.execute("""
            SELECT id, title, content, content_type, file_path, external_url, tags, 
                   embedding_vector, created_at
            FROM knowledge_items
            WHERE embedding_vector IS NOT NULL AND embedding_vector != ''
            """)
            
            results = []
            for row in cursor.fetchall():
                item_id, title, content, content_type, file_path, external_url, tags, embedding_json, created_at = row
                
                # è§£æ embedding
                try:
                    item_embedding = json.loads(embedding_json)
                except:
                    continue
                
                # è®¡ç®—ç›¸ä¼¼åº¦
                similarity = self.cosine_similarity(query_embedding, item_embedding)
                
                # è¿‡æ»¤ä½ç›¸ä¼¼åº¦ç»“æœ
                if similarity >= threshold:
                    results.append({
                        'id': item_id,
                        'title': title,
                        'content': content,
                        'content_type': content_type,
                        'file_path': file_path,
                        'external_url': external_url,
                        'tags': tags,
                        'created_at': created_at,
                        'similarity': similarity
                    })
            
            conn.close()
            
            # 3. æŒ‰ç›¸ä¼¼åº¦é™åºæ’åº
            results.sort(key=lambda x: x['similarity'], reverse=True)
            
            # 4. è¿”å› top-k ç»“æœ
            return results[:limit]
        
        except Exception as e:
            print(f"å‘é‡æœç´¢å¤±è´¥: {e}")
            # å›é€€åˆ°å…³é”®è¯æœç´¢
            return self.search_knowledge(query, limit)
    
    def hybrid_search(self, query: str, limit: int = 5) -> List[Dict]:
        """
        æ··åˆæœç´¢ï¼šç»“åˆå…³é”®è¯æœç´¢å’Œå‘é‡æœç´¢
        
        ç­–ç•¥ï¼š
        1. å…ˆè¿›è¡Œå‘é‡æœç´¢ï¼ˆè¯­ä¹‰åŒ¹é…ï¼‰
        2. å¦‚æœå‘é‡æœç´¢ç»“æœä¸è¶³ï¼Œè¡¥å……å…³é”®è¯æœç´¢ç»“æœ
        3. å»é‡å¹¶è¿”å›
        """
        try:
            # 1. å‘é‡æœç´¢
            vector_results = self.vector_search(query, limit=limit, threshold=0.5)
            
            # 2. å¦‚æœå‘é‡æœç´¢ç»“æœå……è¶³ï¼Œç›´æ¥è¿”å›
            if len(vector_results) >= limit:
                return vector_results
            
            # 3. è¡¥å……å…³é”®è¯æœç´¢
            keyword_results = self.search_knowledge(query, limit=limit * 2)
            
            # 4. åˆå¹¶ç»“æœï¼ˆå»é‡ï¼‰
            seen_ids = {item['id'] for item in vector_results}
            combined_results = vector_results.copy()
            
            for item in keyword_results:
                if item['id'] not in seen_ids and len(combined_results) < limit:
                    # ä¸ºå…³é”®è¯æœç´¢ç»“æœæ·»åŠ ä¸€ä¸ªé»˜è®¤ç›¸ä¼¼åº¦
                    item['similarity'] = 0.3
                    combined_results.append(item)
                    seen_ids.add(item['id'])
            
            return combined_results
        
        except Exception as e:
            print(f"æ··åˆæœç´¢å¤±è´¥: {e}")
            return self.search_knowledge(query, limit)


"""
GuardNova çŸ¥è¯†åº“ç³»ç»Ÿ
æ”¯æŒ RAGï¼ˆRetrieval Augmented Generationï¼‰æ™ºèƒ½æ£€ç´¢
"""

import sqlite3
import json
from datetime import datetime
from pathlib import Path
import hashlib
from typing import List, Dict, Optional
import requests
from bs4 import BeautifulSoup
import pandas as pd
from docx import Document
import PyPDF2

class KnowledgeBase:
    def __init__(self, db_path: str = "data/knowledge_base.db"):
        self.db_path = Path(db_path)
        self.db_path.parent.mkdir(parents=True, exist_ok=True)
        self._init_database()
    
    def _init_database(self):
        """åˆå§‹åŒ–æ•°æ®åº“"""
        conn = sqlite3.connect(self.db_path)
        cursor = conn.cursor()
        
        # çŸ¥è¯†åº“ä¸»è¡¨
        cursor.execute("""
        CREATE TABLE IF NOT EXISTS knowledge_items (
            id INTEGER PRIMARY KEY AUTOINCREMENT,
            title TEXT NOT NULL,
            content TEXT NOT NULL,
            content_type TEXT NOT NULL,
            file_path TEXT,
            external_url TEXT,
            tags TEXT,
            embedding_vector TEXT,
            created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
            updated_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
            last_crawled_at TIMESTAMP
        )
        """)
        
        # åˆ›å»ºç´¢å¼•
        cursor.execute("""
        CREATE INDEX IF NOT EXISTS idx_content_type 
        ON knowledge_items(content_type)
        """)
        
        cursor.execute("""
        CREATE INDEX IF NOT EXISTS idx_tags 
        ON knowledge_items(tags)
        """)
        
        conn.commit()
        conn.close()
    
    def add_text_knowledge(self, title: str, content: str, tags: str = "") -> int:
        """æ·»åŠ æ–‡æœ¬çŸ¥è¯†"""
        conn = sqlite3.connect(self.db_path)
        cursor = conn.cursor()
        
        cursor.execute("""
        INSERT INTO knowledge_items (title, content, content_type, tags)
        VALUES (?, ?, ?, ?)
        """, (title, content, "text", tags))
        
        knowledge_id = cursor.lastrowid
        conn.commit()
        conn.close()
        
        return knowledge_id
    
    def add_file_knowledge(self, title: str, file_path: str, description: str = "", tags: str = "") -> int:
        """æ·»åŠ æ–‡ä»¶çŸ¥è¯†å¹¶è§£æå†…å®¹"""
        conn = sqlite3.connect(self.db_path)
        cursor = conn.cursor()
        
        # è§£ææ–‡ä»¶å†…å®¹
        content = self._parse_file_content(file_path, description)
        
        cursor.execute("""
        INSERT INTO knowledge_items (title, content, content_type, file_path, tags)
        VALUES (?, ?, ?, ?, ?)
        """, (title, content, "file", file_path, tags))
        
        knowledge_id = cursor.lastrowid
        conn.commit()
        conn.close()
        
        return knowledge_id
    
    def _parse_file_content(self, file_path: str, description: str = "") -> str:
        """è§£ææ–‡ä»¶å†…å®¹ä¸ºMarkdownæ ¼å¼"""
        try:
            file_path_obj = Path(file_path)
            suffix = file_path_obj.suffix.lower()
            
            # Excel æ–‡ä»¶
            if suffix in ['.xlsx', '.xls', '.csv']:
                return self._parse_excel(file_path_obj, description)
            
            # Word æ–‡ä»¶
            elif suffix in ['.docx', '.doc']:
                return self._parse_word(file_path_obj, description)
            
            # PDF æ–‡ä»¶
            elif suffix == '.pdf':
                return self._parse_pdf(file_path_obj, description)
            
            # æ–‡æœ¬æ–‡ä»¶
            elif suffix in ['.txt', '.md']:
                with open(file_path, 'r', encoding='utf-8') as f:
                    content = f.read()
                if description:
                    return f"{description}\n\n{content}"
                return content
            
            else:
                return description or f"æ–‡ä»¶ï¼š{file_path_obj.name}"
        
        except Exception as e:
            print(f"è§£ææ–‡ä»¶å¤±è´¥ï¼š{e}")
            return description or f"æ–‡ä»¶ï¼š{file_path}"
    
    def _parse_excel(self, file_path: Path, description: str = "") -> str:
        """è§£æExcelæ–‡ä»¶ä¸ºMarkdownè¡¨æ ¼"""
        try:
            # è¯»å–Excelæ–‡ä»¶
            if file_path.suffix == '.csv':
                df = pd.read_csv(file_path)
            else:
                df = pd.read_excel(file_path)
            
            # è½¬æ¢ä¸ºMarkdownæ ¼å¼
            markdown_parts = []
            
            if description:
                markdown_parts.append(f"{description}\n")
            
            markdown_parts.append(f"**æ–‡ä»¶åï¼š** {file_path.name}")
            markdown_parts.append(f"**æ•°æ®è¡Œæ•°ï¼š** {len(df)} è¡Œ")
            markdown_parts.append(f"**åˆ—æ•°ï¼š** {len(df.columns)} åˆ—")
            markdown_parts.append(f"**åˆ—åï¼š** {', '.join(df.columns.tolist())}\n")
            
            # è½¬æ¢ä¸ºMarkdownè¡¨æ ¼ï¼ˆé™åˆ¶å‰20è¡Œï¼Œé¿å…å†…å®¹è¿‡é•¿ï¼‰
            display_rows = min(20, len(df))
            markdown_table = df.head(display_rows).to_markdown(index=False)
            
            markdown_parts.append("**æ•°æ®å†…å®¹ï¼š**\n")
            markdown_parts.append(markdown_table)
            
            if len(df) > display_rows:
                markdown_parts.append(f"\n*ï¼ˆè¡¨æ ¼å…± {len(df)} è¡Œï¼Œä»¥ä¸Šæ˜¾ç¤ºå‰ {display_rows} è¡Œï¼‰*")
            
            # æ·»åŠ æ•°æ®æ‘˜è¦
            markdown_parts.append("\n**æ•°æ®æ‘˜è¦ï¼š**")
            for col in df.columns[:5]:  # åªæ˜¾ç¤ºå‰5åˆ—çš„æ‘˜è¦
                if df[col].dtype in ['int64', 'float64']:
                    markdown_parts.append(f"- {col}: æ•°å€¼èŒƒå›´ {df[col].min()} ~ {df[col].max()}")
                else:
                    unique_count = df[col].nunique()
                    markdown_parts.append(f"- {col}: {unique_count} ä¸ªä¸åŒå€¼")
            
            return "\n".join(markdown_parts)
        
        except Exception as e:
            return f"{description}\n\næ–‡ä»¶è§£æå¤±è´¥ï¼š{str(e)}" if description else f"Excelæ–‡ä»¶ï¼š{file_path.name}ï¼ˆè§£æå¤±è´¥ï¼‰"
    
    def _parse_word(self, file_path: Path, description: str = "") -> str:
        """è§£æWordæ–‡æ¡£å†…å®¹"""
        try:
            doc = Document(file_path)
            
            markdown_parts = []
            
            if description:
                markdown_parts.append(f"{description}\n")
            
            markdown_parts.append(f"**æ–‡ä»¶åï¼š** {file_path.name}\n")
            
            # æå–æ–‡æœ¬å†…å®¹
            full_text = []
            for para in doc.paragraphs:
                if para.text.strip():
                    full_text.append(para.text)
            
            if full_text:
                markdown_parts.append("\n**æ–‡æ¡£å†…å®¹ï¼š**\n")
                markdown_parts.append("\n\n".join(full_text))
            
            return "\n".join(markdown_parts)
        
        except Exception as e:
            return f"{description}\n\næ–‡ä»¶è§£æå¤±è´¥ï¼š{str(e)}" if description else f"Wordæ–‡æ¡£ï¼š{file_path.name}ï¼ˆè§£æå¤±è´¥ï¼‰"
    
    def _parse_pdf(self, file_path: Path, description: str = "") -> str:
        """è§£æPDFæ–‡ä»¶å†…å®¹"""
        try:
            markdown_parts = []
            
            if description:
                markdown_parts.append(f"{description}\n")
            
            markdown_parts.append(f"**æ–‡ä»¶åï¼š** {file_path.name}\n")
            
            # è¯»å–PDF
            with open(file_path, 'rb') as f:
                reader = PyPDF2.PdfReader(f)
                num_pages = len(reader.pages)
                
                markdown_parts.append(f"**é¡µæ•°ï¼š** {num_pages}\n")
                
                # æå–å‰10é¡µæ–‡æœ¬
                text_parts = []
                for i in range(min(10, num_pages)):
                    page = reader.pages[i]
                    text = page.extract_text()
                    if text.strip():
                        text_parts.append(f"### ç¬¬ {i+1} é¡µ\n{text}")
                
                if text_parts:
                    markdown_parts.append("\n**æ–‡æ¡£å†…å®¹ï¼š**\n")
                    markdown_parts.append("\n\n".join(text_parts))
                
                if num_pages > 10:
                    markdown_parts.append(f"\n\n*ï¼ˆå…± {num_pages} é¡µï¼Œä»…æ˜¾ç¤ºå‰10é¡µï¼‰*")
            
            return "\n".join(markdown_parts)
        
        except Exception as e:
            return f"{description}\n\næ–‡ä»¶è§£æå¤±è´¥ï¼š{str(e)}" if description else f"PDFæ–‡ä»¶ï¼š{file_path.name}ï¼ˆè§£æå¤±è´¥ï¼‰"
    
    def add_url_knowledge(self, title: str, url: str, description: str = "", tags: str = "") -> int:
        """æ·»åŠ é“¾æ¥çŸ¥è¯†ï¼ˆRAG ç½‘é¡µçˆ¬å–ï¼‰"""
        conn = sqlite3.connect(self.db_path)
        cursor = conn.cursor()
        
        # çˆ¬å–ç½‘é¡µå†…å®¹
        content = self._crawl_webpage(url)
        if not content:
            content = description or f"é“¾æ¥ï¼š{url}"
        
        cursor.execute("""
        INSERT INTO knowledge_items (title, content, content_type, external_url, tags, last_crawled_at)
        VALUES (?, ?, ?, ?, ?, ?)
        """, (title, content, "url", url, tags, datetime.now()))
        
        knowledge_id = cursor.lastrowid
        conn.commit()
        conn.close()
        
        return knowledge_id
    
    def _crawl_webpage(self, url: str) -> str:
        """çˆ¬å–ç½‘é¡µå†…å®¹ï¼ˆç®€åŒ–ç‰ˆ RAGï¼‰"""
        try:
            headers = {
                'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'
            }
            response = requests.get(url, headers=headers, timeout=10)
            response.raise_for_status()
            
            soup = BeautifulSoup(response.content, 'html.parser')
            
            # ç§»é™¤è„šæœ¬å’Œæ ·å¼
            for script in soup(["script", "style"]):
                script.decompose()
            
            # æå–æ–‡æœ¬
            text = soup.get_text(separator='\n', strip=True)
            
            # æ¸…ç†ç©ºè¡Œ
            lines = [line.strip() for line in text.split('\n') if line.strip()]
            content = '\n'.join(lines[:100])  # é™åˆ¶é•¿åº¦
            
            return content
        except Exception as e:
            print(f"çˆ¬å–ç½‘é¡µå¤±è´¥ï¼š{e}")
            return ""
    
    def refresh_url_knowledge(self, knowledge_id: int) -> bool:
        """åˆ·æ–°é“¾æ¥çŸ¥è¯†ï¼ˆå®šæ—¶æ›´æ–°ï¼‰"""
        conn = sqlite3.connect(self.db_path)
        cursor = conn.cursor()
        
        cursor.execute("""
        SELECT external_url FROM knowledge_items WHERE id = ? AND content_type = 'url'
        """, (knowledge_id,))
        
        result = cursor.fetchone()
        if not result:
            conn.close()
            return False
        
        url = result[0]
        new_content = self._crawl_webpage(url)
        
        if new_content:
            cursor.execute("""
            UPDATE knowledge_items 
            SET content = ?, updated_at = ?, last_crawled_at = ?
            WHERE id = ?
            """, (new_content, datetime.now(), datetime.now(), knowledge_id))
            
            conn.commit()
            conn.close()
            return True
        
        conn.close()
        return False
    
    def search_knowledge(self, query: str, limit: int = 5) -> List[Dict]:
        """æ™ºèƒ½æœç´¢çŸ¥è¯†åº“ï¼ˆç®€åŒ–ç‰ˆ RAGï¼‰"""
        conn = sqlite3.connect(self.db_path)
        cursor = conn.cursor()
        
        # æ”¹è¿›çš„åˆ†è¯ç­–ç•¥ï¼šè¿‡æ»¤åœç”¨è¯ï¼Œæå–å…³é”®è¯
        stopwords = {'æ˜¯', 'çš„', 'äº†', 'åœ¨', 'æœ‰', 'å’Œ', 'å°±', 'ä¸', 'äºº', 'æˆ‘', 'ä»–', 'å¥¹', 'ä»¬',
                     'ä»€ä¹ˆ', 'æ€ä¹ˆ', 'å¦‚ä½•', 'ä¸ºä»€ä¹ˆ', 'å—', 'å‘¢', 'å•Š', 'å§', 'ä¹ˆ', 'å“ª', 'å“ªé‡Œ',
                     'ä¸€ä¸ª', 'è¿™ä¸ª', 'é‚£ä¸ª', 'è¿™äº›', 'é‚£äº›', 'ä»€', 'ä¹ˆ', 'æ„æ€', 'å®šä¹‰'}
        
        # åˆ†è¯å¹¶è¿‡æ»¤
        words = query.lower().split()
        keywords = [w for w in words if w not in stopwords and len(w) > 1]
        
        # å¦‚æœè¿‡æ»¤åæ²¡æœ‰å…³é”®è¯ï¼Œä½¿ç”¨åŸå§‹æŸ¥è¯¢
        if not keywords:
            keywords = [query.lower()]
        
        # æ„å»ºæœç´¢æ¡ä»¶ï¼ˆä½¿ç”¨ ORï¼Œä½†æ¯ä¸ªå…³é”®è¯éƒ½è¦åŒ¹é…åˆ°æ ‡é¢˜ã€å†…å®¹æˆ–æ ‡ç­¾ï¼‰
        search_conditions = []
        search_params = []
        
        for keyword in keywords:
            search_conditions.append(
                "(LOWER(title) LIKE ? OR LOWER(content) LIKE ? OR LOWER(tags) LIKE ?)"
            )
            search_params.extend([f"%{keyword}%", f"%{keyword}%", f"%{keyword}%"])
        
        where_clause = " OR ".join(search_conditions)
        
        cursor.execute(f"""
        SELECT id, title, content, content_type, file_path, external_url, tags, created_at
        FROM knowledge_items
        WHERE {where_clause}
        ORDER BY updated_at DESC
        LIMIT ?
        """, search_params + [limit])
        
        results = []
        for row in cursor.fetchall():
            results.append({
                'id': row[0],
                'title': row[1],
                'content': row[2],
                'content_type': row[3],
                'file_path': row[4],
                'external_url': row[5],
                'tags': row[6],
                'created_at': row[7]
            })
        
        conn.close()
        return results
    
    def get_all_knowledge(self) -> List[Dict]:
        """è·å–æ‰€æœ‰çŸ¥è¯†"""
        conn = sqlite3.connect(self.db_path)
        cursor = conn.cursor()
        
        cursor.execute("""
        SELECT id, title, content, content_type, file_path, external_url, tags, created_at
        FROM knowledge_items
        ORDER BY created_at DESC
        """)
        
        results = []
        for row in cursor.fetchall():
            results.append({
                'id': row[0],
                'title': row[1],
                'content': row[2],
                'content_type': row[3],
                'file_path': row[4],
                'external_url': row[5],
                'tags': row[6],
                'created_at': row[7]
            })
        
        conn.close()
        return results
    
    def delete_knowledge(self, knowledge_id: int) -> bool:
        """åˆ é™¤çŸ¥è¯†"""
        conn = sqlite3.connect(self.db_path)
        cursor = conn.cursor()
        
        cursor.execute("DELETE FROM knowledge_items WHERE id = ?", (knowledge_id,))
        
        success = cursor.rowcount > 0
        conn.commit()
        conn.close()
        
        return success
    
    def generate_rag_response(self, query: str, ai_client, model: str = "deepseek-chat") -> str:
        """ä½¿ç”¨ RAG ç”Ÿæˆå›ç­”"""
        # 1. æœç´¢çŸ¥è¯†åº“
        search_results = self.search_knowledge(query, limit=3)
        
        # 2. å¦‚æœçŸ¥è¯†åº“ä¸­æœ‰ç›¸å…³å†…å®¹ï¼Œä½¿ç”¨ RAG
        if search_results:
            context = "\n\n".join([
                f"çŸ¥è¯† {i+1}ï¼š{item['title']}\n{item['content'][:500]}"
                for i, item in enumerate(search_results)
            ])
            
            system_prompt = f"""ä½ æ˜¯ GuardNova AI æ™ºèƒ½åŠ©æ‰‹ã€‚

ä»¥ä¸‹æ˜¯ä»çŸ¥è¯†åº“ä¸­æ£€ç´¢åˆ°çš„ç›¸å…³ä¿¡æ¯ï¼š

{context}

è¯·åŸºäºä¸Šè¿°çŸ¥è¯†åº“å†…å®¹å›ç­”ç”¨æˆ·çš„é—®é¢˜ã€‚å¦‚æœçŸ¥è¯†åº“å†…å®¹ä¸é—®é¢˜ç›¸å…³ï¼Œè¯·ä¼˜å…ˆä½¿ç”¨çŸ¥è¯†åº“ä¿¡æ¯ï¼›å¦‚æœçŸ¥è¯†åº“å†…å®¹ä¸è¶³ä»¥å›ç­”é—®é¢˜ï¼Œè¯·ç»“åˆä½ çš„çŸ¥è¯†è¿›è¡Œè¡¥å……è¯´æ˜ã€‚

è¯·ç”¨å‹å¥½ã€ä¸“ä¸šçš„è¯­æ°”å›ç­”ï¼Œå¹¶åœ¨é€‚å½“æ—¶æ ‡æ³¨ä¿¡æ¯æ¥æºï¼ˆçŸ¥è¯†åº“æˆ–é€šç”¨çŸ¥è¯†ï¼‰ã€‚"""
        else:
            # 3. çŸ¥è¯†åº“ä¸­æ²¡æœ‰ç›¸å…³å†…å®¹ï¼Œä½¿ç”¨é€šç”¨ AI
            system_prompt = "ä½ æ˜¯ GuardNovaï¼Œä¸€ä¸ªä¸“ä¸šã€å‹å¥½çš„ AI æ™ºèƒ½åŠ©æ‰‹ã€‚"
        
        # 4. è°ƒç”¨ AI ç”Ÿæˆå›ç­”
        try:
            messages = [
                {"role": "system", "content": system_prompt},
                {"role": "user", "content": query}
            ]
            
            response = ai_client.chat.completions.create(
                model=model,
                messages=messages,
                temperature=0.7,
                max_tokens=2000
            )
            
            answer = response.choices[0].message.content
            
            # 5. æ·»åŠ çŸ¥è¯†åº“æ¥æºæ ‡æ³¨
            if search_results:
                sources = "\n\n---\nğŸ“š **å‚è€ƒçŸ¥è¯†ï¼š**\n" + "\n".join([
                    f"- {item['title']}" for item in search_results
                ])
                answer += sources
            
            return answer
        except Exception as e:
            return f"æŠ±æ­‰ï¼Œç”Ÿæˆå›ç­”æ—¶å‡ºç°é”™è¯¯ï¼š{str(e)}"


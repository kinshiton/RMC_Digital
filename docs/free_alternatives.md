# ðŸ†“ å…è´¹æ›¿ä»£æ–¹æ¡ˆæŒ‡å—

æœ¬æ–‡æ¡£æä¾›AzureæœåŠ¡çš„å…è´¹æ›¿ä»£æ–¹æ¡ˆï¼Œè®©æ‚¨æ— éœ€ä»»ä½•è´¹ç”¨å³å¯è¿è¡Œæ™ºèƒ½å®‰é˜²ç³»ç»Ÿã€‚

## æ–¹æ¡ˆå¯¹æ¯”

| åŠŸèƒ½ | AzureæœåŠ¡ | å…è´¹æ›¿ä»£æ–¹æ¡ˆ | ä¼˜ç¼ºç‚¹ |
|------|----------|-------------|--------|
| **LLMå¯¹è¯** | Azure OpenAI (ä»˜è´¹) | Ollamaæœ¬åœ° / Geminiå…è´¹ | æœ¬åœ°ï¼šå®Œå…¨å…è´¹ä½†éœ€GPUï¼›Geminiï¼šå…è´¹ä½†æœ‰é™é¢ |
| **è§†è§‰åˆ†æž** | Azure Vision (ä»˜è´¹) | OpenCVæœ¬åœ° / Hugging Faceå…è´¹ | OpenCVï¼šå…è´¹ä½†åŠŸèƒ½åŸºç¡€ï¼›HFï¼šå…è´¹ä½†è¾ƒæ…¢ |
| **æ–‡æœ¬åˆ†æž** | Azure Language (ä»˜è´¹) | æœ¬åœ°è§„åˆ™å¼•æ“Ž / TextBlob | å®Œå…¨å…è´¹ï¼Œå‡†ç¡®åº¦ç•¥ä½Ž |

---

## ðŸŽ¯ æŽ¨èæ–¹æ¡ˆï¼šOllamaæœ¬åœ° + è§„åˆ™å¼•æ“Ž

### ä¼˜åŠ¿
- âœ… **å®Œå…¨å…è´¹**ï¼Œæ— APIè´¹ç”¨
- âœ… **æ•°æ®éšç§**ï¼Œæ‰€æœ‰å¤„ç†åœ¨æœ¬åœ°
- âœ… **æ— é™é¢**ï¼Œä¸å—è°ƒç”¨æ¬¡æ•°é™åˆ¶
- âœ… **ç¦»çº¿è¿è¡Œ**ï¼Œæ— éœ€ç½‘ç»œ

### ç¡¬ä»¶è¦æ±‚
- **æœ€ä½Žé…ç½®**ï¼š8GB RAMï¼ˆå¯è¿è¡Œ7Bæ¨¡åž‹ï¼‰
- **æŽ¨èé…ç½®**ï¼š16GB RAM + GPUï¼ˆå¯è¿è¡Œ13Bæ¨¡åž‹ï¼‰
- **æ‚¨çš„Mac**ï¼šç¬¦åˆè¦æ±‚ï¼Œå¯ä»¥è¿è¡Œï¼

---

## ðŸ“¦ å®‰è£…Ollamaï¼ˆæœ¬åœ°LLMï¼‰

### 1. å®‰è£…Ollama

```bash
# macOSå®‰è£…ï¼ˆæŽ¨èï¼‰
curl -fsSL https://ollama.com/install.sh | sh

# æˆ–è®¿é—® https://ollama.com/download ä¸‹è½½å®‰è£…åŒ…
```

### 2. ä¸‹è½½æ¨¡åž‹

```bash
# ä¸‹è½½è½»é‡çº§æ¨¡åž‹ï¼ˆæŽ¨èï¼š7Bå‚æ•°ï¼Œçº¦4GBï¼‰
ollama pull llama3.2:3b

# æˆ–ä¸‹è½½æ›´å¼ºå¤§çš„æ¨¡åž‹ï¼ˆéœ€è¦æ›´å¤šå†…å­˜ï¼‰
ollama pull llama3.1:8b

# ä¸­æ–‡ä¼˜åŒ–æ¨¡åž‹
ollama pull qwen2.5:7b
```

### 3. æµ‹è¯•è¿è¡Œ

```bash
# å¯åŠ¨OllamaæœåŠ¡
ollama serve

# æ–°ç»ˆç«¯æµ‹è¯•
ollama run llama3.2:3b "ä½ å¥½ï¼Œæµ‹è¯•ä¸€ä¸‹"
```

### 4. APIè°ƒç”¨æµ‹è¯•

```bash
curl http://localhost:11434/api/generate -d '{
  "model": "llama3.2:3b",
  "prompt": "åˆ†æžè¿™ä¸ªæŠ¥è­¦ï¼šé—¨ç¦å¼‚å¸¸",
  "stream": false
}'
```

---

## ðŸ–¼ï¸ è§†è§‰åˆ†æžå…è´¹æ–¹æ¡ˆ

### æ–¹æ¡ˆ1ï¼šçº¯OpenCVï¼ˆå®Œå…¨å…è´¹ï¼Œå·²å®‰è£…ï¼‰

ä¼˜åŠ¿ï¼š
- âœ… å®Œå…¨æœ¬åœ°ï¼Œæ— éœ€API
- âœ… è¿åŠ¨æ£€æµ‹ã€è½®å»“è¯†åˆ«
- âœ… å·²é›†æˆåœ¨ä»£ç ä¸­

é™åˆ¶ï¼š
- âš ï¸ æ— æ³•è¯†åˆ«å¯¹è±¡ç±»åž‹ï¼ˆåªèƒ½æ£€æµ‹è¿åŠ¨ï¼‰

### æ–¹æ¡ˆ2ï¼šHugging Face Inference APIï¼ˆå…è´¹ï¼‰

```bash
# å®‰è£…ä¾èµ–
pip install transformers pillow torch

# æ— éœ€APIå¯†é’¥ï¼Œå…è´¹ä½¿ç”¨
```

ä»£ç ç¤ºä¾‹ï¼š
```python
from transformers import pipeline

# ä½¿ç”¨å…è´¹çš„å¯¹è±¡æ£€æµ‹æ¨¡åž‹
detector = pipeline("object-detection", model="facebook/detr-resnet-50")

# åˆ†æžå›¾åƒ
results = detector("path/to/image.jpg")
print(results)  # [{'label': 'person', 'score': 0.99, ...}]
```

---

## ðŸ“ æ–‡æœ¬åˆ†æžå…è´¹æ–¹æ¡ˆ

### æ–¹æ¡ˆ1ï¼šè§„åˆ™å¼•æ“Žï¼ˆå·²å®žçŽ°ï¼‰

ç³»ç»Ÿå·²å†…ç½®è§„åˆ™å¼•æ“Žï¼Œæ— éœ€Azureï¼š
- å…³é”®è¯åŒ¹é…ï¼ˆé«˜é£Žé™©è¯ã€ä¸­é£Žé™©è¯ï¼‰
- æ—¶é—´å› ç´ ï¼ˆå¤œé—´ã€å‘¨æœ«ï¼‰
- åœ°ç‚¹å› ç´ ï¼ˆå…³é”®åŒºåŸŸã€å…¬å…±åŒºåŸŸï¼‰

### æ–¹æ¡ˆ2ï¼šTextBlobï¼ˆå…è´¹æƒ…æ„Ÿåˆ†æžï¼‰

```bash
pip install textblob
python -m textblob.download_corpora
```

ä»£ç ç¤ºä¾‹ï¼š
```python
from textblob import TextBlob

text = "é—¨ç¦ç³»ç»Ÿæ£€æµ‹åˆ°æœªæŽˆæƒè®¿é—®å°è¯•"
blob = TextBlob(text)
sentiment = blob.sentiment.polarity  # -1åˆ°1ï¼Œè´Ÿå€¼è¡¨ç¤ºè´Ÿé¢
```

---

## ðŸ”„ é…ç½®ç³»ç»Ÿä½¿ç”¨å…è´¹æ–¹æ¡ˆ

### åˆ›å»ºå…è´¹é…ç½®æ–‡ä»¶

åˆ›å»º `.env.free` æ–‡ä»¶ï¼š

```bash
# ä½¿ç”¨Ollamaæœ¬åœ°LLM
USE_LOCAL_LLM=true
OLLAMA_BASE_URL=http://localhost:11434
OLLAMA_MODEL=llama3.2:3b

# è§†è§‰åˆ†æžï¼šä»…ä½¿ç”¨OpenCVï¼ˆæ— éœ€APIï¼‰
USE_OPENCV_ONLY=true
ENABLE_AZURE_VISION=false

# æ–‡æœ¬åˆ†æžï¼šä½¿ç”¨è§„åˆ™å¼•æ“Ž
USE_RULE_BASED_RISK=true
ENABLE_AZURE_LANGUAGE=false

# ä¸éœ€è¦ä»»ä½•APIå¯†é’¥ï¼
OPENAI_API_KEY=
AZURE_OPENAI_ENDPOINT=
AZURE_OPENAI_API_KEY=
```

### ä½¿ç”¨å…è´¹é…ç½®å¯åŠ¨

```bash
# å¤åˆ¶å…è´¹é…ç½®
cp .env.free .env

# å¯åŠ¨ç³»ç»Ÿ
python app/main.py
```

---

## ðŸ’» ä¿®æ”¹ä»£ç æ”¯æŒOllama

åˆ›å»º `modules/llm_adapter.py`ï¼ˆé€‚é…å¤šç§LLMåŽç«¯ï¼‰ï¼š

```python
import os
import requests
from typing import Dict

class LLMAdapter:
    """LLMé€‚é…å™¨ï¼Œæ”¯æŒå¤šç§åŽç«¯"""
    
    def __init__(self):
        self.use_local = os.getenv('USE_LOCAL_LLM', 'false').lower() == 'true'
        self.ollama_url = os.getenv('OLLAMA_BASE_URL', 'http://localhost:11434')
        self.ollama_model = os.getenv('OLLAMA_MODEL', 'llama3.2:3b')
    
    def generate(self, prompt: str) -> str:
        """ç”Ÿæˆæ–‡æœ¬"""
        if self.use_local:
            return self._ollama_generate(prompt)
        else:
            # Azure OpenAIæˆ–å…¶ä»–
            return self._azure_generate(prompt)
    
    def _ollama_generate(self, prompt: str) -> str:
        """ä½¿ç”¨Ollamaæœ¬åœ°ç”Ÿæˆ"""
        response = requests.post(
            f"{self.ollama_url}/api/generate",
            json={
                "model": self.ollama_model,
                "prompt": prompt,
                "stream": False
            }
        )
        return response.json()['response']
    
    def _azure_generate(self, prompt: str) -> str:
        """ä½¿ç”¨Azure OpenAI"""
        # åŽŸæœ‰Azureé€»è¾‘
        pass

# ä½¿ç”¨ç¤ºä¾‹
llm = LLMAdapter()
result = llm.generate("åˆ†æžè¿™ä¸ªæŠ¥è­¦ï¼šé—¨ç¦å¼‚å¸¸")
```

---

## ðŸŒ å…¶ä»–å…è´¹APIé€‰é¡¹

### 1. Google Geminiï¼ˆå…è´¹é¢åº¦ï¼‰

- **å…è´¹é¢åº¦**ï¼šæ¯åˆ†é’Ÿ60æ¬¡è°ƒç”¨
- **æ³¨å†Œ**ï¼šhttps://makersuite.google.com/app/apikey

```bash
pip install google-generativeai

# é…ç½®
export GOOGLE_API_KEY="your_free_key"
```

```python
import google.generativeai as genai

genai.configure(api_key=os.getenv('GOOGLE_API_KEY'))
model = genai.GenerativeModel('gemini-pro')

response = model.generate_content("åˆ†æžæŠ¥è­¦ï¼šé—¨ç¦å¼‚å¸¸")
print(response.text)
```

### 2. Groqï¼ˆå…è´¹é«˜é€ŸæŽ¨ç†ï¼‰

- **å…è´¹é¢åº¦**ï¼šæ¯å¤©14,400æ¬¡è°ƒç”¨
- **é€Ÿåº¦æžå¿«**ï¼šæ¯”OpenAIå¿«5-10å€
- **æ³¨å†Œ**ï¼šhttps://console.groq.com/

```bash
pip install groq

export GROQ_API_KEY="your_free_key"
```

```python
from groq import Groq

client = Groq(api_key=os.getenv('GROQ_API_KEY'))
response = client.chat.completions.create(
    model="llama-3.1-8b-instant",
    messages=[{"role": "user", "content": "åˆ†æžæŠ¥è­¦"}]
)
print(response.choices[0].message.content)
```

### 3. Hugging Face Inference APIï¼ˆå…è´¹ï¼‰

- **å®Œå…¨å…è´¹**ï¼Œæ— éœ€ä¿¡ç”¨å¡
- **é€Ÿåº¦è¾ƒæ…¢**ï¼Œé€‚åˆæ‰¹å¤„ç†

```bash
pip install huggingface-hub
```

```python
from huggingface_hub import InferenceClient

client = InferenceClient()
response = client.text_generation(
    "åˆ†æžæŠ¥è­¦ï¼šé—¨ç¦å¼‚å¸¸",
    model="meta-llama/Llama-3.2-3B-Instruct"
)
```

---

## ðŸ“Š æ€§èƒ½å¯¹æ¯”

| æ–¹æ¡ˆ | æˆæœ¬ | é€Ÿåº¦ | éšç§ | ç¦»çº¿ | æŽ¨èåº¦ |
|------|------|------|------|------|--------|
| **Ollamaæœ¬åœ°** | å…è´¹ | å¿« | â­â­â­â­â­ | âœ… | â­â­â­â­â­ |
| **Groqå…è´¹API** | å…è´¹ | æžå¿« | â­â­â­ | âŒ | â­â­â­â­ |
| **Geminiå…è´¹** | å…è´¹ | ä¸­ | â­â­â­ | âŒ | â­â­â­â­ |
| **è§„åˆ™å¼•æ“Ž** | å…è´¹ | æžå¿« | â­â­â­â­â­ | âœ… | â­â­â­ |
| Azureä»˜è´¹ | $$$ | å¿« | â­â­â­ | âŒ | â­â­â­â­ |

---

## ðŸŽ¯ æˆ‘çš„æŽ¨èé…ç½®ï¼ˆå®Œå…¨å…è´¹ï¼‰

### åœºæ™¯1ï¼šé‡è§†éšç§ï¼Œæœ‰è¾ƒå¥½ç¡¬ä»¶

```bash
# ä½¿ç”¨Ollamaæœ¬åœ°
USE_LOCAL_LLM=true
OLLAMA_MODEL=qwen2.5:7b  # ä¸­æ–‡æ”¯æŒå¥½

# è§†è§‰ï¼šOpenCV
USE_OPENCV_ONLY=true

# æ–‡æœ¬åˆ†æžï¼šè§„åˆ™å¼•æ“Ž
USE_RULE_BASED_RISK=true
```

**ä¼˜åŠ¿**ï¼šå®Œå…¨ç¦»çº¿ï¼Œæ•°æ®ä¸å‡ºæœ¬åœ°

### åœºæ™¯2ï¼šç¡¬ä»¶ä¸€èˆ¬ï¼Œéœ€è¦è¾ƒå¥½æ•ˆæžœ

```bash
# ä½¿ç”¨Groqå…è´¹APIï¼ˆé€Ÿåº¦å¿«ï¼‰
USE_GROQ=true
GROQ_API_KEY=your_free_key
GROQ_MODEL=llama-3.1-8b-instant

# è§†è§‰ï¼šOpenCV + HuggingFace
USE_OPENCV_ONLY=false
USE_HF_VISION=true

# æ–‡æœ¬åˆ†æžï¼šGeminiå…è´¹
USE_GEMINI=true
GOOGLE_API_KEY=your_free_key
```

**ä¼˜åŠ¿**ï¼šå…è´¹ä¸”æ•ˆæžœå¥½ï¼Œé€Ÿåº¦å¿«

---

## âš¡ å¿«é€Ÿå¯åŠ¨ï¼ˆå®Œå…¨å…è´¹ç‰ˆï¼‰

```bash
# 1. å®‰è£…Ollama
curl -fsSL https://ollama.com/install.sh | sh

# 2. ä¸‹è½½æ¨¡åž‹
ollama pull qwen2.5:7b

# 3. å¯åŠ¨OllamaæœåŠ¡
ollama serve &

# 4. é…ç½®çŽ¯å¢ƒå˜é‡
cat > .env << EOF
USE_LOCAL_LLM=true
OLLAMA_BASE_URL=http://localhost:11434
OLLAMA_MODEL=qwen2.5:7b
USE_OPENCV_ONLY=true
USE_RULE_BASED_RISK=true
EOF

# 5. å¯åŠ¨ç³»ç»Ÿ
python app/main.py
```

---

## ðŸ“ž éœ€è¦å¸®åŠ©ï¼Ÿ

å¦‚æžœæ‚¨é€‰æ‹©å…è´¹æ–¹æ¡ˆä½†ä¸ç¡®å®šå¦‚ä½•é…ç½®ï¼Œå‘Šè¯‰æˆ‘ï¼š
1. æ‚¨çš„ç¡¬ä»¶é…ç½®ï¼ˆRAMã€æ˜¯å¦æœ‰GPUï¼‰
2. æ˜¯å¦éœ€è¦ç¦»çº¿è¿è¡Œ
3. å¯¹å“åº”é€Ÿåº¦çš„è¦æ±‚

æˆ‘ä¼šä¸ºæ‚¨æŽ¨èæœ€ä½³é…ç½®æ–¹æ¡ˆï¼

---

**æ€»ç»“**ï¼šæŽ¨èä½¿ç”¨ **Ollamaæœ¬åœ°** æ–¹æ¡ˆï¼Œå®Œå…¨å…è´¹ã€éšç§å®‰å…¨ã€æ— é™é¢ä½¿ç”¨ï¼

